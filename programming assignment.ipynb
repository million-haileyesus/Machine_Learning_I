{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77ecfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8d89d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c4557",
   "metadata": {},
   "source": [
    "# Traditional Features Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcad6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image():\n",
    "    path = \"../DataSet/Training/Normal\"\n",
    "    folders = os.listdir(path)\n",
    "    no_of_images = []\n",
    "    for folder in folders:\n",
    "        files = [f for f in os.listdir(f\"{path}/{folder}\") if os.path.isfile(os.path.join(f\"../Resized/{folder}\", f))]\n",
    "        no_of_images.append(len(files))\n",
    "        \n",
    "    collection = {}\n",
    "    for folder, count in zip(folders, no_of_images):\n",
    "        all_files = os.listdir(f\"{path}/{folder}\")\n",
    "        collection[folder] = all_files\n",
    "        \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae65a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_collections = get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef3473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(path, new_path, folder, image):\n",
    "    path = path + \"/\" + folder + \"/\" + image\n",
    "    loaded_image = cv.imread(path)\n",
    "    gray = cv.cvtColor(loaded_image, cv.COLOR_BGR2GRAY)\n",
    "    sift = cv.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(gray, None)\n",
    "    img = cv.drawKeypoints(gray, kp, loaded_image)\n",
    "    new_path = new_path + \"/\" + folder\n",
    "    if not os.path.exists(new_path):\n",
    "        os.mkdir(new_path)\n",
    "    new_path = new_path + \"/\" + image\n",
    "    cv.imwrite(new_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe77cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features():\n",
    "    new_path = \"../DataSet/Training/Feature Extracted\"\n",
    "    path = \"../DataSet/Training/Normal\"\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for folder, images in image_collections.items():\n",
    "        for image in images:\n",
    "            feature_extraction(path, new_path, folder, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f888dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08dd8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = glob.glob(\"../DataSet/Training/Feature Extracted/*/*\")\n",
    "testing_images = glob.glob(\"../DataSet/Testing/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b49aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels(images):\n",
    "    combine = []\n",
    "    for img in images:\n",
    "        loaded_image = cv.imread(img)\n",
    "        combine.append(loaded_image)\n",
    "    \n",
    "    return combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde96a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = get_pixels(training_images)\n",
    "testing_images = get_pixels(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd001b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(X):\n",
    "    kmeans = KMeans(n_clusters=100, random_state=0, init=\"random\", n_init=\"auto\").fit(X)\n",
    "    \n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e849ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_reduction(features):\n",
    "    pca = PCA(n_components=2)\n",
    "    transformed_data = pca.fit_transform(features)\n",
    "    \n",
    "    for i in range(4):\n",
    "        c = np.flatnonzero(class_labels == i)\n",
    "        plt.scatter(transformed_data[c, 0], transformed_data[c, 1])\n",
    "    plt.show()\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8e6bc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ffb18",
   "metadata": {},
   "source": [
    "# Traditional Machine Learnign model - Support Vecotr Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(kernel_type=\"linear\"):\n",
    "    parameters = [0.001, 0.1, 1.0, 10, 100]\n",
    "    \n",
    "    for c in parameters:\n",
    "        svc = SVC(kernel=kernel_type, C=c, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe09f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_performance():\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef86c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_for_different_kernels():\n",
    "    kernel_type = \"rbf\", \"poly\", \"sigmoid\"\n",
    "    for kernel in kernel_type:\n",
    "        train_svm(kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b593f82",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9498655",
   "metadata": {},
   "source": [
    "# Deep Learning - Training a Simple Convolution Neural Network Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c93a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_classifier():\n",
    "    num_nodes = 10\n",
    "    input_shape = (100, 100, 1)\n",
    "    \n",
    "    model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape), \n",
    "        layers.Conv2D(8, kernel_size=(3, 3), activation=\"relu\"), \n",
    "        layers.Conv2D(8, kernel_size=(3, 3), activation=\"relu\"), \n",
    "        layers.MaxPooling2D(pool_size=(2, 2)), \n",
    "        layers.Flatten(), \n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(num_nodes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e5f5c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 98, 98, 8)         80        \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 96, 96, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 48, 48, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                294928    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 296034 (1.13 MB)\n",
      "Trainable params: 296034 (1.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = images_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbc2f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    batch_size = 128\n",
    "    num_epochs = 30\n",
    "    X_train, y_train, X_test, y_test = train_test_split(training_images, testing_images, test_size=0.2)\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 100, 100, 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 100, 100, 1))\n",
    "    \n",
    "    y_train = to_categorical(y_train, num_classes=10)\n",
    "    y_test = to_categorical(y_test, num_classes=10)\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=0.2)\n",
    "    \n",
    "    return model, history, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5f01d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation():\n",
    "    model, history, X_test, y_test = train_model()\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"\\n\\nTest loss:\", score[0])\n",
    "    print(\"Test accuracy:\", score[1])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8b3b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\noill\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\noill\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 98, 98, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 96, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 48, 48, 8)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18432)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                294928    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 296034 (1.13 MB)\n",
      "Trainable params: 296034 (1.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'standardized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m, in \u001b[0;36mmodel_evaluation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_evaluation\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     model, history, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, score[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m images_classifier()\n\u001b[1;32m----> 5\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mstandardized_dataset\u001b[49m()\n\u001b[0;32m      7\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(X_train, (X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      8\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(X_test, (X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'standardized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "model = model_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1035be",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a927",
   "metadata": {},
   "source": [
    "# Transfer Learning via Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b08bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
